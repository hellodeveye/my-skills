#!/usr/bin/env python3
"""
Tech News Generator - æ¯æ—¥ç§‘æŠ€æ–°é—»èšåˆå™¨

åŠŸèƒ½ï¼š
- ä»å¤šæºæŠ“å–ç§‘æŠ€æ–°é—»
- AIç¿»è¯‘ä¸ºä¸­æ–‡
- ç²¾é€‰10æ¡ï¼Œå‡è¡¡å„æ¥æº
- ä¸‹è½½é…å›¾ä¸Šä¼ åˆ°R2
- ç”ŸæˆMarkdownæ±‡æ€»

ç”¨æ³•ï¼š
  python3 generate.py                          # é»˜è®¤ç”Ÿæˆ
  python3 generate.py --output-only            # ä»…è¾“å‡ºç”Ÿæˆçš„Markdown
  python3 generate.py --save ~/news.md         # ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶
  python3 generate.py --no-images              # ä¸å¤„ç†å›¾ç‰‡
"""

import argparse
import json
import os
import re
import subprocess
import sys
import urllib.request
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urljoin

# é…ç½®
SCRIPT_DIR = Path(__file__).parent.resolve()
CACHE_DIR = SCRIPT_DIR.parent / "cache"
CACHE_DIR.mkdir(exist_ok=True)

FETCH_NEWS = SCRIPT_DIR / "fetch_news.py"

# åˆ†ç±»é…ç½®
CATEGORIES = {
    "AI ä¸æœºå™¨å­¦ä¹ ": ["ai", "llm", "model", "agent", "gpt", "claude", "grok", "ml ", "neural", "deep learning", "machine learning", "huggingface", "transformer"],
    "å¼€å‘å·¥å…·ä¸å¼€æº": ["rust", "python", "javascript", "typescript", "github", "open source", "framework", "library", "tool", "compiler", "database", "sql", "redis"],
    "åŸºç¡€è®¾æ–½ä¸äº‘åŸç”Ÿ": ["cloud", "aws", "gcp", "azure", "server", "datacenter", "infrastructure", "kubernetes", "docker", "devops", "security", "privacy", "encryption", "observability"],
    "äº§å“ä¸è®¾è®¡": ["product", "design", "ui", "ux", "figma", "startup", "launch", "feature", "update", "release"],
    "è¶£é—»ä¸è§‚ç‚¹": [],  # é»˜è®¤åˆ†ç±»
}

DEFAULT_SOURCES = ["hackernews", "github-trending", "lobsters", "devto"]


def fetch_multi_sources(sources, count=15):
    """ä»å¤šä¸ªæºæŠ“å–æ–°é—»ã€‚"""
    tmp = CACHE_DIR / "fetched_news.json"
    cmd = [sys.executable, str(FETCH_NEWS), "--sources", *sources, "--count", str(count), "--output", str(tmp)]
    subprocess.run(cmd, check=True, capture_output=True)
    return json.loads(tmp.read_text(encoding="utf-8"))


def categorize(title):
    """æ ¹æ®æ ‡é¢˜å…³é”®è¯åˆ†ç±»ã€‚"""
    t = title.lower()
    for category, keywords in CATEGORIES.items():
        if any(k in t for k in keywords):
            return category
    return "è¶£é—»ä¸è§‚ç‚¹"


def translate_with_llm(title, description, source_name=None):
    """ä½¿ç”¨LLMç¿»è¯‘æ ‡é¢˜å’Œç”Ÿæˆæ‘˜è¦ã€‚"""
    import os

    try:
        minimax_key = os.environ.get('MINIMAX_API_KEY', '').strip()
        openai_key = os.environ.get('OPENAI_API_KEY', '').strip()
        has_api_key = bool(minimax_key or openai_key)

        if has_api_key:
            sys.path.insert(0, str(SCRIPT_DIR))
            from llm_translate import translate_title_and_summary, TranslationError
            return translate_title_and_summary(title, description=description or "", source=source_name)
    except TranslationError as e:
        print(f"[ç¿»è¯‘è­¦å‘Š] {e}", file=sys.stderr)
    except Exception as e:
        print(f"[ç¿»è¯‘é”™è¯¯] {type(e).__name__}", file=sys.stderr)

    # å›é€€ï¼šç®€å•å¤„ç†
    return title, f"æ¥è‡ª {source_name or 'ç§‘æŠ€ç¤¾åŒº'} çš„çƒ­é—¨å†…å®¹ã€‚\n\nè¦ç‚¹ï¼š\n- è¯¦æƒ…è§åŸæ–‡\n- å€¼å¾—å…³æ³¨"


def load_translation_cache():
    """åŠ è½½ç¿»è¯‘ç¼“å­˜ã€‚"""
    cache_path = CACHE_DIR / "translations.json"
    if cache_path.exists():
        return json.loads(cache_path.read_text(encoding="utf-8"))
    return {}


def save_translation_cache(cache):
    """ä¿å­˜ç¿»è¯‘ç¼“å­˜ã€‚"""
    cache_path = CACHE_DIR / "translations.json"
    cache_path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding="utf-8")


def dedupe_articles(articles, days=3):
    """åŸºäºç¼“å­˜å»é‡æœ€è¿‘Nå¤©çš„æ–‡ç« ã€‚"""
    cache_path = CACHE_DIR / "selected_articles.json"
    seen_links = set()

    if cache_path.exists():
        history = json.loads(cache_path.read_text(encoding="utf-8"))
        cutoff = (datetime.now() - timedelta(days=days)).isoformat()
        for entry in history:
            if entry.get("date", "") > cutoff:
                seen_links.add(entry.get("link"))

    return [a for a in articles if a.get("link") not in seen_links]


def pick_articles_balanced(articles, limit=10, per_source=2):
    """å‡è¡¡é€‰æ‹©æ–‡ç« ï¼Œç¡®ä¿æ¥æºå¤šæ ·æ€§ã€‚"""
    buckets = {}
    for a in articles:
        buckets.setdefault(a.get('source', 'unknown'), []).append(a)

    source_order = list(buckets.keys())
    picked = []

    # ç¬¬ä¸€è½®ï¼šæ¯æºæœ€å¤š2æ¡
    for src in source_order:
        if src in buckets:
            picked.extend(buckets[src][:per_source])
        if len(picked) >= limit:
            return picked[:limit]

    # ç¬¬äºŒè½®ï¼šè½®è¯¢è¡¥å……
    i = per_source
    while len(picked) < limit:
        progressed = False
        for src in source_order:
            if src in buckets and i < len(buckets[src]):
                picked.append(buckets[src][i])
                progressed = True
                if len(picked) >= limit:
                    return picked[:limit]
        if not progressed:
            break
        i += 1

    return picked[:limit]


def fetch_og_image(url, timeout=5):
    """æŠ“å–æ–‡ç« çš„og:imageã€‚"""
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            html = resp.read().decode("utf-8", errors="ignore")

        # å°è¯•å¤šç§og:imageæ ¼å¼
        patterns = [
            r'<meta[^>]*property=["\']og:image["\'][^>]*content=["\']([^"\']+)["\']',
            r'<meta[^>]*content=["\']([^"\']+)["\'][^>]*property=["\']og:image["\']',
            r'<meta[^>]*name=["\']twitter:image["\'][^>]*content=["\']([^"\']+)["\']',
        ]

        for pattern in patterns:
            match = re.search(pattern, html, re.I)
            if match:
                return urljoin(url, match.group(1))
    except Exception:
        pass
    return None


def upload_image_to_r2(image_url, key, timeout=10):
    """ä¸Šä¼ å›¾ç‰‡åˆ°R2å¹¶è¿”å›å…¬å¼€URLã€‚"""
    # é€šè¿‡ç¯å¢ƒå˜é‡æˆ–ç›¸å¯¹è·¯å¾„æŸ¥æ‰¾ r2-upload
    r2_path = os.environ.get("R2_UPLOAD_PATH")
    if r2_path:
        paths_to_try = [r2_path]
    else:
        # é»˜è®¤å°è¯•ï¼šåŒçº§ç›®å½•ä¸‹çš„ r2-upload
        paths_to_try = [
            str(SCRIPT_DIR.parent.parent / "r2-upload" / "scripts"),
        ]

    for path in paths_to_try:
        if path not in sys.path:
            sys.path.insert(0, path)

    try:
        from upload import fetch_and_upload
        return fetch_and_upload(image_url, key=key, make_public=True)
    except ImportError:
        print(f"[è­¦å‘Š] r2-upload ä¸å¯ç”¨ï¼Œè·³è¿‡: {image_url}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"[ä¸Šä¼ å¤±è´¥] {image_url}: {e}", file=sys.stderr)
        return None


def process_articles_images(articles, date_str, max_images=10):
    """å¤„ç†æ–‡ç« å›¾ç‰‡ï¼Œä¸Šä¼ R2å¹¶è¿”å›URLåˆ—è¡¨ã€‚"""
    uploaded_urls = []

    for i, article in enumerate(articles):
        if len(uploaded_urls) >= max_images:
            article.pop("image_url", None)
            continue

        # æŠ“å–og:image
        image_url = fetch_og_image(article["link"])
        if not image_url:
            article.pop("image_url", None)
            continue

        # ç”ŸæˆR2 key
        key = f"images/{date_str.replace('-', '/')}/article-{i+1:02d}.jpg"

        # ä¸Šä¼ åˆ°R2
        public_url = upload_image_to_r2(image_url, key)
        if public_url:
            article["image_url"] = public_url
            uploaded_urls.append({
                "article": article.get("zh_title", article["title"])[:30],
                "r2_url": public_url,
                "source_image": image_url
            })
            print(f"  [å›¾ç‰‡ä¸Šä¼ ] {public_url}")
        else:
            article.pop("image_url", None)

    return uploaded_urls


def generate_markdown(date_str, articles):
    """ç”ŸæˆMarkdownæ ¼å¼çš„æ–°é—»æ±‡æ€»ï¼Œä½¿ç”¨å›ºå®šæ ¼å¼ã€‚"""
    lines = []

    # 1. å›ºå®šæ ‡é¢˜æ ¼å¼
    lines.append(f"# ğŸ“° {date_str} ç§‘æŠ€æ—©æŠ¥")
    lines.append("")

    # 2. å›ºå®šæ‘˜è¦æ ¼å¼ - åŒ…å«æ–‡ç« æ•°é‡å’Œæ¥æºåˆ†å¸ƒ
    source_counts = {}
    for a in articles:
        src = a.get("source_name", a.get("source", "æœªçŸ¥"))
        source_counts[src] = source_counts.get(src, 0) + 1

    source_summary = " | ".join([f"{src}({count})" for src, count in sorted(source_counts.items())])

    lines.append("> ğŸ“Š **ä»Šæ—¥å¯¼è¯»**")
    lines.append(f"> ç²¾é€‰ {len(articles)} æ¡ç§‘æŠ€æ–°é—»")
    lines.append(f"> æ¥æºï¼š{source_summary}")
    lines.append("")
    lines.append("---")
    lines.append("")

    # 3. æ–‡ç« é€Ÿè§ˆ - å›ºå®šæ ¼å¼çš„ç›®å½•
    lines.append("## ğŸ“‹ æ–‡ç« é€Ÿè§ˆ")
    lines.append("")

    # æŒ‰åˆ†ç±»åˆ†ç»„ç»Ÿè®¡
    grouped = {cat: [] for cat in CATEGORIES.keys()}
    for article in articles:
        cat = article.get("category", categorize(article["title"]))
        if cat not in grouped:
            cat = "è¶£é—»ä¸è§‚ç‚¹"
        grouped[cat].append(article)

    # ç”Ÿæˆåˆ†ç±»æ¦‚è§ˆ
    for category in CATEGORIES.keys():
        items = grouped[category]
        if not items:
            continue
        lines.append(f"**{category}**ï¼š{len(items)} ç¯‡")
        for i, item in enumerate(items, 1):
            zh_title = item.get("zh_title", item["title"])
            # é™åˆ¶æ ‡é¢˜é•¿åº¦
            display_title = zh_title[:40] + "..." if len(zh_title) > 40 else zh_title
            lines.append(f"{i}. {display_title}")
        lines.append("")

    lines.append("---")
    lines.append("")

    # 4. è¯¦ç»†å†…å®¹ - å›ºå®šç»“æ„
    for category in CATEGORIES.keys():
        items = grouped[category]
        if not items:
            continue

        lines.append(f"## {category}")
        lines.append("")

        for idx, item in enumerate(items, 1):
            zh_title = item.get("zh_title", item["title"])
            zh_summary = item.get("zh_summary", "")
            source_name = item.get("source_name", item.get("source", "æ¥æº"))

            # å›ºå®šæ–‡ç« ç¼–å·æ ¼å¼
            lines.append(f"### {idx}. {zh_title}")
            lines.append("")

            # å…ƒä¿¡æ¯è¡Œ - å›ºå®šæ ¼å¼
            meta_info = f"ğŸ“° **{source_name}**"
            lines.append(meta_info)
            lines.append("")

            # å›¾ç‰‡ - å›ºå®šä½ç½®
            if item.get("image_url"):
                lines.append(f'<img src="{item["image_url"]}" width="100%" alt="{zh_title[:20]}" style="border-radius:8px;margin:10px 0;">')
                lines.append("")

            # æ‘˜è¦å†…å®¹ - å›ºå®šæ ¼å¼å¤„ç†
            if zh_summary:
                # è§£ææ‘˜è¦å’Œè¦ç‚¹
                summary_parts = zh_summary.split("\n\nè¦ç‚¹ï¼š")
                main_summary = summary_parts[0].strip()

                if main_summary:
                    lines.append("**æ‘˜è¦**ï¼š" + main_summary)
                    lines.append("")

                # è¦ç‚¹å¤„ç†
                if len(summary_parts) > 1:
                    bullet_text = summary_parts[1].strip()
                    bullets = [b.strip() for b in bullet_text.split("\n") if b.strip().startswith("-")]

                    if bullets:
                        lines.append("**æ ¸å¿ƒè¦ç‚¹**ï¼š")
                        for bullet in bullets[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªè¦ç‚¹
                            # ç§»é™¤å¼€å¤´çš„ "- "
                            bullet_content = bullet[2:].strip() if bullet.startswith("- ") else bullet
                            lines.append(f"â€¢ {bullet_content}")
                        lines.append("")

            # åŸæ–‡é“¾æ¥ - å›ºå®šæ ¼å¼
            lines.append(f"ğŸ”— [é˜…è¯»åŸæ–‡]({item['link']})")
            lines.append("")
            lines.append("---")
            lines.append("")

    lines.append(f"*æœ¬æ¬¡æ±‡æ€»äº {datetime.now().strftime('%Y-%m-%d %H:%M')} ç”Ÿæˆ*")
    lines.append("")

    return "\n".join(lines)


def save_selected_history(articles, date_str):
    """ä¿å­˜å·²é€‰æ–‡ç« åˆ°å†å²è®°å½•ï¼ˆç”¨äºå»é‡ï¼‰ã€‚"""
    cache_path = CACHE_DIR / "selected_articles.json"
    history = []
    if cache_path.exists():
        history = json.loads(cache_path.read_text(encoding="utf-8"))

    for a in articles:
        history.append({
            "date": datetime.now().isoformat(),
            "link": a["link"],
            "title": a.get("zh_title", a["title"]),
        })

    # åªä¿ç•™æœ€è¿‘30å¤©
    cutoff = (datetime.now() - timedelta(days=30)).isoformat()
    history = [h for h in history if h.get("date", "") > cutoff]

    cache_path.write_text(json.dumps(history, ensure_ascii=False, indent=2), encoding="utf-8")


def print_summary(articles, uploaded_images, elapsed_time):
    """æ‰“å°æ‰§è¡Œæ‘˜è¦ã€‚"""
    print("\n" + "="*60)
    print("æ‰§è¡Œæ‘˜è¦")
    print("="*60)

    # ç»Ÿè®¡
    source_count = {}
    for a in articles:
        src = a.get("source_name", a.get("source", "unknown"))
        source_count[src] = source_count.get(src, 0) + 1

    print(f"\næ–‡ç« ç»Ÿè®¡:")
    print(f"  - ç²¾é€‰æ–‡ç« : {len(articles)} æ¡")
    print(f"  - ä¸Šä¼ å›¾ç‰‡: {len(uploaded_images)} å¼ ")
    print(f"  - è€—æ—¶: {elapsed_time:.1f} ç§’")

    print(f"\næ¥æºåˆ†å¸ƒ:")
    for src, count in sorted(source_count.items(), key=lambda x: -x[1]):
        print(f"  - {src}: {count} æ¡")

    if uploaded_images:
        print(f"\nå›¾ç‰‡URLåˆ—è¡¨:")
        for img in uploaded_images:
            print(f"  - {img['article'][:25]}...")
            print(f"    {img['r2_url']}")

    print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆç§‘æŠ€æ–°é—»æ±‡æ€»")
    parser.add_argument("--date", help="æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä»Šå¤©")
    parser.add_argument("--sources", nargs="+", default=DEFAULT_SOURCES, help="æ–°é—»æºåˆ—è¡¨")
    parser.add_argument("--count", type=int, default=15, help="æ¯æºæŠ“å–æ•°é‡")
    parser.add_argument("--limit", type=int, default=10, help="æœ€ç»ˆç²¾é€‰æ•°é‡")
    parser.add_argument("--max-images", type=int, default=10, help="æœ€å¤§å›¾ç‰‡æ•°")
    parser.add_argument("--no-images", action="store_true", help="ä¸å¤„ç†å›¾ç‰‡")
    parser.add_argument("--save", help="ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-only", action="store_true", help="ä»…è¾“å‡ºç”Ÿæˆçš„Markdown")

    args = parser.parse_args()

    import time
    start_time = time.time()

    # æ—¥æœŸ
    date_str = args.date or datetime.now().strftime("%Y-%m-%d")

    # 1. æŠ“å–æ–°é—»
    print(f"[1/5] ä» {len(args.sources)} ä¸ªæºæŠ“å–æ–°é—»...")
    articles = fetch_multi_sources(args.sources, args.count)
    print(f"      è·å– {len(articles)} ç¯‡æ–‡ç« ")

    # 2. å»é‡
    print("[2/5] å»é™¤è¿‘æœŸé‡å¤æ–‡ç« ...")
    articles = dedupe_articles(articles)
    print(f"      å‰©ä½™ {len(articles)} ç¯‡")

    # 3. ç²¾é€‰
    print(f"[3/5] ç²¾é€‰ {args.limit} ç¯‡æ–‡ç« ...")
    articles = pick_articles_balanced(articles, limit=args.limit)
    print(f"      å·²ç²¾é€‰: {', '.join(a.get('source', 'unknown') for a in articles)}")

    # 4. ç¿»è¯‘
    print("[4/5] ç¿»è¯‘æ ‡é¢˜å’Œç”Ÿæˆæ‘˜è¦...")
    cache = load_translation_cache()
    for a in articles:
        key = a.get("link") or a.get("title")
        if key in cache:
            a["zh_title"] = cache[key].get("zh_title")
            a["zh_summary"] = cache[key].get("zh_summary")
        else:
            zh_title, zh_summary = translate_with_llm(
                a.get("title", ""),
                a.get("description"),
                source_name=a.get("source_name")
            )
            a["zh_title"] = zh_title
            a["zh_summary"] = zh_summary
            cache[key] = {"zh_title": zh_title, "zh_summary": zh_summary}
    save_translation_cache(cache)

    # 5. å›¾ç‰‡å¤„ç†
    uploaded_images = []
    if not args.no_images:
        print("[5/5] æŠ“å–å¹¶ä¸Šä¼ æ–‡ç« é…å›¾...")
        uploaded_images = process_articles_images(articles, date_str, args.max_images)
        print(f"      å·²ä¸Šä¼  {len(uploaded_images)} å¼ å›¾ç‰‡")
    else:
        print("[5/5] è·³è¿‡å›¾ç‰‡å¤„ç†")

    # 6. ç”ŸæˆMarkdown
    markdown = generate_markdown(date_str, articles)

    # 7. ä¿å­˜å†å²ï¼ˆç”¨äºå»é‡ï¼‰
    save_selected_history(articles, date_str)

    # 8. è¾“å‡ºç»“æœ
    if args.output_only:
        print(markdown)
    elif args.save:
        Path(args.save).write_text(markdown, encoding="utf-8")
        print(f"\nå·²ä¿å­˜åˆ°: {args.save}")
    else:
        print("\n" + "="*60)
        print("ç”Ÿæˆçš„å†…å®¹")
        print("="*60)
        print(markdown)

    # 9. æ‰§è¡Œæ‘˜è¦
    elapsed = time.time() - start_time
    print_summary(articles, uploaded_images, elapsed)

    return {
        "markdown": markdown,
        "articles": articles,
        "images": uploaded_images,
    }


if __name__ == "__main__":
    result = main()
